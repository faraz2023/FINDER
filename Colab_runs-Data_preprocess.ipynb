{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "he2Ai6krWato",
   "metadata": {
    "id": "he2Ai6krWato"
   },
   "source": [
    "## This notebook shows the data preprocess code, can run directly from Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38807a64-9c23-4555-9f65-db46b8968f94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38807a64-9c23-4555-9f65-db46b8968f94",
    "outputId": "fa816ed2-108b-4e12-9478-3bad6928f966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount GD\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# your GD path to clone the repo\n",
    "project_path=\"/content/drive/MyDrive/UofT_MEng/MIE1517/Project/FINDER_github/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Xke1xrMKpTo",
   "metadata": {
    "id": "0Xke1xrMKpTo"
   },
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "%cd {project_path}\n",
    "\n",
    "!git clone https://github.com/faraz2023/FINDER-pytorch.git\n",
    "\n",
    "%cd FINDER-pytorch\n",
    "%ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uF_ThUe4LJ1l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uF_ThUe4LJ1l",
    "outputId": "4f011e6a-011b-44c5-d3c2-7161c0384765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/UofT_MEng/MIE1517/Project/FINDER_github/FINDER-pytorch\n",
      "/content/drive/MyDrive/UofT_MEng/MIE1517/Project/FINDER_github/FINDER-pytorch\n"
     ]
    }
   ],
   "source": [
    "# if already cloned\n",
    "%cd {project_path}/FINDER-pytorch/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df41064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx==2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461de6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  If not GD, run from local starts here ##############\n",
    "project_path=\"/FINDER_CM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99657487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic line for local jupyter notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65342c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw HI-II-14 data from .tsv format\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# The data and web portal are made available to the public under the CC BY 4.0 license. \n",
    "# Users of the web portal or its data should cite the web portal and the HuRI publication.\n",
    "# Data source: http://www.interactome-atlas.org/\n",
    "# HuRI publication: https://pubmed.ncbi.nlm.nih.gov/25416956/\n",
    "\n",
    "import os.path\n",
    "\n",
    "# gather other protein networks\n",
    "datasets = [\"H-I-05\",\"HI-II-14\",\"HuRI\",\"HI-union\",\"Venkatesan-09\",\"Yang-16\",\"Test_space_screens-19\",\"Yu-11\",\"Lit-BM\"]\n",
    "\n",
    "for dateset_name in datasets:\n",
    "    data_url = f\"http://www.interactome-atlas.org/data/{dateset_name}.tsv\"\n",
    "    save_dir_g = f\"{project_path}/FINDER-pytorch/data/real/{dateset_name}.txt\"\n",
    "\n",
    "    if(not os.path.isfile(save_dir_g)):\n",
    "        raw_edge_list = pd.read_csv(data_url, sep='\\t', names=['node_from','node_to'])\n",
    "\n",
    "        # As we can see there are several self referencing edges, we would need to clean up those first\n",
    "        uni_edge_list = raw_edge_list[raw_edge_list.node_from != raw_edge_list.node_to]\n",
    "\n",
    "        # Now we need to mask all protein labels [GENCODE (v27)] such as ENSG00000204889 to index numbers\n",
    "        edge_list = uni_edge_list.stack().rank(method='dense').unstack().astype(int)\n",
    "\n",
    "        # Then we need to un-scale all index by 1, so it starts at 0\n",
    "        edge_list['node_from']-=1\n",
    "        edge_list['node_to']-=1\n",
    "\n",
    "        # Now we use networkx lib to convert it into a graph\n",
    "        G = nx.from_pandas_edgelist(edge_list, source='node_from', target='node_to')\n",
    "\n",
    "        # We add weights to nodes (note it's not weights on edges)\n",
    "        nx.set_node_attributes(G, 0.0, \"weight\")\n",
    "\n",
    "        # write to edgelist file\n",
    "        nx.write_edgelist(G, save_dir_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e729dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source (strictly-new dataset)\n",
    "\n",
    "data_url_union = f\"http://www.interactome-atlas.org/data/HI-union.tsv\"\n",
    "data_url_14 = f\"http://www.interactome-atlas.org/data/HI-II-14.tsv\"\n",
    "\n",
    "save_dir_g = f\"{project_path}/FINDER-pytorch/data/real/HI-union-exclude-14.txt\"\n",
    "\n",
    "if(not os.path.isfile(save_dir_g)):\n",
    "    raw_edge_list_union = pd.read_csv(data_url_union, sep='\\t', names=['node_from','node_to'])\n",
    "    raw_edge_list_14 = pd.read_csv(data_url_14, sep='\\t', names=['node_from','node_to'])\n",
    "\n",
    "    # As we can see there are several self referencing edges, we would need to clean up those first\n",
    "    uni_edge_list_union = raw_edge_list_union[raw_edge_list_union.node_from != raw_edge_list_union.node_to]\n",
    "    uni_edge_list_14 = raw_edge_list_14[raw_edge_list_14.node_from != raw_edge_list_14.node_to]\n",
    "    \n",
    "    # get HI-union - HI-II-14 by protein labels [GENCODE (v27)]\n",
    "    node_from_mask_1 = uni_edge_list_union.node_from.isin(uni_edge_list_14.node_from)\n",
    "    node_from_mask_2 = uni_edge_list_union.node_from.isin(uni_edge_list_14.node_to)\n",
    "    \n",
    "    node_to_mask_1 = uni_edge_list_union.node_to.isin(uni_edge_list_14.node_from)\n",
    "    node_to_mask_2 = uni_edge_list_union.node_to.isin(uni_edge_list_14.node_to)\n",
    "    \n",
    "    node_mask = ~ ( ( node_from_mask_1 | node_from_mask_2 ) | ( node_to_mask_1 | node_to_mask_2 ) )\n",
    "    \n",
    "    uni_edge_list = uni_edge_list_union[node_mask]\n",
    "    \n",
    "    # Now we need to mask all protein labels [GENCODE (v27)] such as ENSG00000204889 to index numbers\n",
    "    edge_list = uni_edge_list.stack().rank(method='dense').unstack().astype(int)\n",
    "    print(edge_list.sort_values(by=['node_from']))\n",
    "\n",
    "    # Then we need to un-scale all index by 1, so it starts at 0\n",
    "    edge_list['node_from']-=1\n",
    "    edge_list['node_to']-=1\n",
    "\n",
    "    # Now we use networkx lib to convert it into a graph\n",
    "    G = nx.from_pandas_edgelist(edge_list, source='node_from', target='node_to')\n",
    "\n",
    "    # We add weights to nodes (note it's not weights on edges)\n",
    "    nx.set_node_attributes(G, 0.0, \"weight\")\n",
    "\n",
    "    nx.write_edgelist(G, save_dir_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461f3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding weights for ND_cost models\n",
    "\n",
    "import os.path\n",
    "\n",
    "# build degree weight for ND_cost models\n",
    "datasets = [\"H-I-05\",\"HI-II-14\",\"HuRI\",\"HI-union\",\"Venkatesan-09\",\"Yang-16\",\"Test_space_screens-19\",\"Yu-11\",\"Lit-BM\",\"HI-union-exclude-14\"]\n",
    "\n",
    "for dateset_name in datasets:\n",
    "    g = nx.read_edgelist(f\"{project_path}/FINDER-pytorch/data/real/{dateset_name}.txt\")\n",
    "    degree = nx.degree(g)\n",
    "    maxDegree = max(dict(degree).values())\n",
    "    weights = {}\n",
    "    for node in g.nodes():\n",
    "        weights[node] = degree[node] / maxDegree\n",
    "    \n",
    "    nx.set_node_attributes(g, weights, 'weight')\n",
    "    save_dir_g = f\"{project_path}/FINDER-pytorch/data/real/cost/{dateset_name}_degree.gml\"\n",
    "    if(not os.path.isfile(save_dir_g)):\n",
    "        nx.write_gml(g, save_dir_g)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Colab_runs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
